{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “We manage what we measure, but frequently we measure what is easy.” - Datanuts meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to dataset\n",
    "https://www.kaggle.com/lewisgmorris/warehouse-picking-times\n",
    "\n",
    "Content\n",
    "\n",
    "PH_DOC - unique order reference\n",
    "\n",
    "PH_SORDER - unique order reference\n",
    "\n",
    "PH_DELIVER - unique delivery reference\n",
    "\n",
    "PH_PICKEDB - name of picker\n",
    "\n",
    "PH_PICKSTA - datetime of picking started\n",
    "\n",
    "PH_PICKEND - datetime of picking ended\n",
    "\n",
    "PH_TOTALLI - total lines picked\n",
    "\n",
    "PH_TOTALBO - total boxes used\n",
    "\n",
    "Inspiration\n",
    "\n",
    "Can you find the patterns in the numbers?\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "What days are busiest?   \n",
    "Are things getting better or worse?   \n",
    "Who is the best?   \n",
    "Whos not pulling their weight?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As seen above these are typical questions asked from UPH (units per hour) information. But is this the whole story?     \n",
    "Assuming that a worker is not \"pulling their weight\" because of low UPH assumes that people generally don't want to perform well and puts this burden on the individual to fix the problem. This removes the responsibility from management to find bottle necks and other sources of delay that prevent workers from succeeding and also allows senior managers to erroniously compare disperate environments.\n",
    "\n",
    "#### What if we assumed that every worker wants to succeed and limitations in the environment prevents them from doing so?    \n",
    "This mindset would challenge managers to find and fix the areas and processes that are getting in the way of success and/or to determine if the cost of fixing the issue is more or less expensive than hiring additional labor and would provide a more accurate basis of performance assessment.\n",
    "\n",
    "#### How do we find the bottlenecks and issues that prevent success?    \n",
    "Ask the people doing the job!\n",
    "\n",
    "#### Some frequent limiting factors that are not typically considered when establishing or comparing UPH metrics from site to site:    \n",
    "- Bulk/Lumber distribution: weather, % of warehouse indoor/outdoor, ground level/ramp, state truck maximum weight capacity, product diversity of size, product diversity of weight, max forklift capcity, average drive distance from stacks to truck, asile width of main travel paths, max safe forklift operating speed, average # years operator experience, average # units/truck, % live unload vs staged trucks, lighting levels, equipment down time, in stock %, varibility in product count/package size, average temperature\n",
    "\n",
    "- Indoor operations/non-perishable: product diversity of size, product diversity of weight, max forklift capcity, average drive distance from stacks to truck, asile width of main travel paths, max safe forklift operating speed, average # years operator experience, average # units/truck, % live unload vs staged trucks, floor load/pallet, % full pallet items/individual pick, conveyer lines/pallet jack/forklift pick, products with team lift requirements, sqft of pick zone, average # picks per order, equipment down time, in stock %, varibility in product count/package size, average temperature\n",
    "\n",
    "- Store pulling: store volume, store sqft, product diversity of size, product diversity of weight, order storage area(walk-in/transfer to shelf), sqft of pick zone, average # picks per order, length of shift, task distribution among operators, operator experience(and/or time for response on questions), in stock %, varibility in product count/package size, % use best judgement/do not substitue orders\n",
    "\n",
    "#### Some frequent limiting factors that are not typically considered when establishing or comparing UPH metrics within a site:\n",
    "- Bulk/Lumber distribution: average # years operator experience, average # units/truck, % live unload vs staged trucks, task distribution among operators (inbound, outbound, \"non-productive\"(safety walks, meetings, training), average # units/truck, % live unload vs staged trucks, equipment down time, in stock %, varibility in product count/package size, max forklift capcity, average drive distance from stacks to truck\n",
    "\n",
    "- Indoor operations/non-perishable: product diversity of size, product diversity of weight, max forklift capcity, average drive distance from stacks to truck, asile width of main travel paths, average # years operator experience, average # units/truck, % live unload vs staged trucks, floor load/pallet, % full pallet items/individual pick, conveyer lines/pallet jack/forklift pick, % products with team lift requirements, sqft of pick zone, average # picks per order, equipment down time, in stock %, varibility in product count/package size\n",
    "\n",
    "- Store pulling: sproduct diversity of size, product diversity of weight, sqft of pick zone, average # picks per order, length of shift, task distribution among operators, operator experience(and/or time for response on questions), in stock %, % use best judgement/do not substitue orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally I would find data for all of the above information then explore to see which information showed the greatest impact for predicting units per hour and could build a ML model to predict UPH for a specific site or within a site.\n",
    "\n",
    "Data source ideas:\n",
    "Bulk/Lumber distribution: find my old files, maybe reach out to previous connections to see if any info could be exported\n",
    "\n",
    "Create a fictional warehouse based off of this picking dataset. Use public retail sales dataset for a product mix and generate random info within a range for product weight, size. Use apriori algoithm to generate product frequencies and create fictional orders?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pick data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: drop all Unnamed columns, these contain only null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of useable columns only, PH_Doc and PH_sorder are showing 'value#' for most entries, \n",
    "# PH_deliver contains large # of nulls\n",
    "\n",
    "pickdf = df[['PH_PICKEDB', 'PH_PICKSTA', 'PH_PICKEND', 'PH_TOTALLI', 'PH_TOTALBO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this df to csv for future use\n",
    "#pickdf.to_csv('pickdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf = pd.read_csv('pickdf.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a very small # of nulls, will drop all\n",
    "pickdf.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf = pickdf.rename(columns={'PH_PICKEDB': 'operator', 'PH_PICKSTA': 'start_time', 'PH_PICKEND': 'end_time', 'PH_TOTALLI': 'total_lines', 'PH_TOTALBO': 'total_boxes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert start_time and end_time to date time\n",
    "pickdf['start']= pd.to_datetime(pickdf['start_time'])\n",
    "pickdf['end']= pd.to_datetime(pickdf['end_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickdf = pickdf.drop(columns=['start_time', 'end_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE\n",
    "\n",
    "### For 1st iteration looking to create baseline and model to predict boxes/hr that beats baseline\n",
    "\n",
    "#### for this iteration total lines will be defined as the number of line items on the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import acquire\n",
    "import prepare\n",
    "import wrangle_pick\n",
    "\n",
    "# Exploring\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquire: downloading raw data files...\n",
      "Acquire: Completed!\n",
      "Prepare: Cleaning acquired data...\n",
      "Prepare: Completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((115575, 14), (23995, 14), (20396, 14))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, validate = wrangle_pick.wrangle_pick_data()\n",
    "train.shape, test.shape, validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional info needed time from start to end, day of week, year category\n",
    "# train['pick_time'] = train.end - train.start\n",
    "# train['int_day'] = train.start.dt.dayofweek\n",
    "# train['day_name'] = train.start.dt.day_name()\n",
    "# train['start_year'] = pd.DatetimeIndex(train['start']).year\n",
    "# train['start_month'] = pd.DatetimeIndex(train['start']).month\n",
    "# train['start_month_year'] = pd.to_datetime(train['start']).dt.to_period('M')\n",
    "# train['end_year'] = pd.DatetimeIndex(train['start']).year\n",
    "# train['end_month'] = pd.DatetimeIndex(train['start']).month\n",
    "# train['end_month_year'] = pd.to_datetime(train['start']).dt.to_period('M')\n",
    "# train.head()\n",
    "# add these to prepare.py - done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more data cleaning needed   \n",
    "\n",
    "- sort operator column into actual operators\n",
    "    - drop all operator names that only appear 1 time\n",
    "    - for now remove all operator names that occur less than 10 times\n",
    "    - a list of those has been made as variable one_off if needed later\n",
    "    - one_off operators will need to be removed from full dataframe before modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_list = train.operator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "operdf = pd.DataFrame(op_list)\n",
    "operdf.reset_index()\n",
    "operdf = operdf.rename(columns={'index': 'name', 'operator': 'occur_times'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_off = operdf[operdf.occur_times < 10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_op_list = operdf[operdf.occur_times > 9].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IT', 'DP', 'DACE', 'JK', 'HB', 'AH', 'EDITE', 'IVETA', 'PETER', 'TH',\n",
       "       'EDYTA', 'ANDREA', 'HARRY', 'AS', 'PICKER', 'CB', 'LM', 'RYAN', 'SAM',\n",
       "       'RP', 'PK', 'SJ', 'SD', 'LEWIS', 'JR', 'JS', 'J', 'EQ', 'NB18', 'DANNY',\n",
       "       'DK', 'TS', 'GD', 'HM', 'GREG', 'GK', 'FREDDIE', 'DE', 'TB', 'JOE',\n",
       "       'RT', 'ITT', 'PUKITE', 'ND', 'SR', 'JAMIE', 'EDIT', 'NIKI', 'SOS',\n",
       "       'SOPHIE', 'EWA', 'F-BR6H', 'AW', 'MARK', 'UN3090LABE', 'IVA', 'EW',\n",
       "       'OF', 'DES', 'PETE', 'DR', 'GEMMA', 'IVETTE', 'WH109/18', 'MES3',\n",
       "       'DAACE', 'NB6', 'DEZ', 'NB11', 'PHIL', 'SW10G', 'W109/24', 'TONY',\n",
       "       'ROB', 'W100S/19', ' EDYTA', 'EDITEE', 'HARRY1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_op_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp = train[train.operator.isin(keep_op_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IT          29737\n",
       "DP          17494\n",
       "DACE        14018\n",
       "JK           7777\n",
       "HB           7584\n",
       "            ...  \n",
       "W100S/19       11\n",
       " EDYTA         11\n",
       "ROB            11\n",
       "EDITEE         10\n",
       "HARRY1         10\n",
       "Name: operator, Length: 78, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_exp.operator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exp.to_csv('train_exp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# return to this - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional info needed  operator tenure (in this dataset)\n",
    "op_max_start = train.groupby('operator')['start'].min()\n",
    "op_max_start\n",
    "#train.groupby('operator')['end'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDYTA = train[train.operator == 'EDYTA']\n",
    "EDYTA.sort_values('start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions for explore\n",
    "1. total min/max pick time? by operator? by day?\n",
    "2. does longer tenure = faster pick_time\n",
    "3. does pick_time average vary significantly by day of week\n",
    "4. days with most orders? fewest? is there a change in # of operators on fewest vs highest order days?\n",
    "\n",
    "additional variations\n",
    "- assign some operators are PT (4hr vs 8hr shift)\n",
    "- define each line as 1 unique item in 1 unique location, based on that infer size of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
